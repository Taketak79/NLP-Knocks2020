{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"font.family\"] = \"IPAexGothic\"\n",
    "\n",
    "import pydot_ng as pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自分作成\n",
    "#最初に読み込んだファイルの状態で、forループをしないと、\n",
    "#一語一語という解釈になってしまい、「吾」「輩」というような切れ目になってしまう。\n",
    "with open(\"neko.txt\",\"r\",encoding = \"utf-8_sig\") as f:\n",
    "    with open(\"neko.txt.cabocha\",\"w\",encoding = \"utf-8_sig\") as g:\n",
    "        c = CaboCha.Parser()\n",
    "        for i in f:\n",
    "            g.write(c.parse(i).toString(CaboCha.FORMAT_LATTICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参考サイト1 (neko2.txt.cabochaとして保存)\n",
    "#参考サイト2,自分とは少し異なるが、Byte数は一緒だから同じデータ？？\n",
    "#https://qiita.com/sh05_sh05/items/434ab60449f230f95e77\n",
    "import CaboCha\n",
    "c = CaboCha.Parser()\n",
    "with open('neko.txt',encoding = \"utf-8_sig\") as f:\n",
    "    text = f.read()\n",
    "with open('neko2.txt.cabocha', mode='w',encoding = \"utf-8_sig\") as f:\n",
    "    for se in  [s + '。' for s in text.split('。')]:\n",
    "        f.write(c.parse(se ).toString(CaboCha.FORMAT_LATTICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参考サイト2\n",
    "# https://qiita.com/segavvy/items/2f686cfb0065c8cbe698\n",
    "import CaboCha\n",
    "\n",
    "fname = 'neko.txt'\n",
    "fname_parsed = 'neko3.txt.cabocha'\n",
    "\n",
    "\n",
    "def parse_neko():\n",
    "    '''「吾輩は猫である」を係り受け解析\n",
    "    「吾輩は猫である」(neko.txt)を係り受け解析してneko.txt.cabochaに保存する\n",
    "    '''\n",
    "    with open(fname,encoding = \"utf-8_sig\") as data_file, \\\n",
    "            open(fname_parsed, mode='w',encoding = \"utf-8_sig\") as out_file:\n",
    "\n",
    "        cabocha = CaboCha.Parser()\n",
    "        for line in data_file:\n",
    "            out_file.write(\n",
    "                cabocha.parse(line).toString(CaboCha.FORMAT_LATTICE)\n",
    "            )\n",
    "\n",
    "parse_neko()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第40問"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "配列番号で何番目？2\n",
      "surface:名前\t base:名前\t pos:名詞\t pos1:一般\n",
      "surface:は\t base:は\t pos:助詞\t pos1:係助詞\n",
      "surface:まだ\t base:まだ\t pos:副詞\t pos1:助詞類接続\n",
      "surface:無い\t base:無い\t pos:形容詞\t pos1:自立\n",
      "surface:。\t base:。\t pos:記号\t pos1:句点\n"
     ]
    }
   ],
   "source": [
    "#CaboChaでは、値渡しはやめておく。\n",
    "#fは一応グローバルな変数としておく。\n",
    "#イテレータ・ジェネレータは以下のサイトが詳しい\n",
    "#https://qiita.com/tomotaka_ito/items/35f3eb108f587022fa09\n",
    "\n",
    "with open(\"neko.txt.cabocha\",\"r\",encoding=\"utf-8_sig\") as f:\n",
    "    \n",
    "    class Morph:\n",
    "        #表層形,基本形,品詞,品詞細分類1の順\n",
    "        def __init__(self,surface,base,pos,pos1):\n",
    "            self.surface = surface\n",
    "            self.base = base\n",
    "            self.pos = pos\n",
    "            self.pos1 = pos1\n",
    "    \n",
    "        #イテレータ\n",
    "        def __str__(self):\n",
    "            return \"surface:{}\\t base:{}\\t pos:{}\\t pos1:{}\".format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "    def Analysis_Morph():\n",
    "        morphs = []\n",
    "        flag = 0\n",
    "        #flagを導入した理由\n",
    "        #EOSが2回続いた場合、2回目の場所が空リストとなって実行されてしまうため\n",
    "        \n",
    "        for i in f:\n",
    "        \n",
    "            if i == \"EOS\\n\" and flag > 0:\n",
    "                flag = 0\n",
    "                continue\n",
    "            \n",
    "            elif i == \"EOS\\n\":\n",
    "                #一文が終了したことの処理\n",
    "                flag += 1 \n",
    "                yield morphs #表示の処理を実行\n",
    "                morphs = [] #空リストに戻す\n",
    "            \n",
    "            else:\n",
    "                #*で始まる行はスキップ\n",
    "                if i[0] == \"*\":\n",
    "                    flag = 0\n",
    "                    continue\n",
    "                    \n",
    "                else:\n",
    "                    flag = 0\n",
    "                    ls = i.split(\"\\t\") \n",
    "                    ks = ls[1].split(\",\")\n",
    "                    #ls[0]=surface, ks[0]=base, ks[1]=pos, ks[6]=pos1\n",
    "                    morphs.append(Morph(ls[0],ks[6],ks[0],ks[1]))\n",
    "        \n",
    "        #raise StopIteration() #yield文がなくなった時用の例外処理(無くても大丈夫？)\n",
    "        \n",
    "    k = Analysis_Morph() #リストの作成\n",
    "\n",
    "    l = int(input(\"配列番号で何番目？\"))\n",
    "    \n",
    "    for n,items in enumerate(k):\n",
    "        if n == l: #要素は0番目スタートなので、3番目は配列で言うと2番目\n",
    "            for item in items:\n",
    "                print(item)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第41問"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "配列番号で何番目？3\n",
      "[0]words:　どこで\t dst:1\t srcs:[]\n",
      "[1]words:生れたか\t dst:4\t srcs:[0]\n",
      "[2]words:とんと\t dst:4\t srcs:[]\n",
      "[3]words:見当が\t dst:4\t srcs:[]\n",
      "[4]words:つかぬ。\t dst:-1\t srcs:[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "#CaboChaでは、値渡しはやめておく。\n",
    "#fは一応グローバルな変数としておく。\n",
    "#イテレータ・ジェネレータは以下のサイトが詳しい\n",
    "#https://qiita.com/tomotaka_ito/items/35f3eb108f587022fa09\n",
    "\n",
    "import re\n",
    "\n",
    "with open(\"neko.txt.cabocha\",\"r\",encoding=\"utf-8_sig\") as f:\n",
    "    \n",
    "    class Morph:\n",
    "        #表層形,基本形,品詞,品詞細分類1の順\n",
    "        def __init__(self,surface,base,pos,pos1):\n",
    "            self.surface = surface\n",
    "            self.base = base\n",
    "            self.pos = pos\n",
    "            self.pos1 = pos1\n",
    "    \n",
    "        #イテレータ\n",
    "        def __str__(self):\n",
    "            return \"surface:{}\\t base:{}\\t pos:{}\\t pos1:{}\".format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "    class Chunk:\n",
    "        #形態素（Morphオブジェクト）のリスト,係り先文節インデックス番号,係り元文節インデックス番号のリスト（srcs）\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.morphs = []\n",
    "            self.dst = -1 #index番号は-1としておく\n",
    "            self.srcs = []\n",
    "        \n",
    "        def __str__(self):\n",
    "            surface = \"\"\n",
    "            for morph in self.morphs:\n",
    "                surface += morph.surface\n",
    "            return \"words:{}\\t dst:{}\\t srcs:{}\".format(surface,self.dst,self.srcs)\n",
    "    \n",
    "    def Analysis_Chunk():\n",
    "        chunks = dict()\n",
    "        idx = -1 #keyとなるindex number\n",
    "        flag = 0\n",
    "        #flagを導入した理由\n",
    "        #EOSが2回続いた場合、2回目の場所が空リストとなって実行されてしまうため\n",
    "        \n",
    "        for i in f:\n",
    "        \n",
    "            if i == \"EOS\\n\" and flag > 0:\n",
    "                flag = 0\n",
    "                continue\n",
    "            \n",
    "            elif i == \"EOS\\n\":\n",
    "                #一文が終了したことの処理\n",
    "                flag += 1\n",
    "                \n",
    "                if len(chunks) > 0:\n",
    "                    data = sorted(chunks.items(),key=lambda x:x[0]) \n",
    "                    #このように条件指定することで、配列の0番目がKeyとなりSortしてくれる\n",
    "                    yield list(zip(*data))[1] #2データ以上ある時は、zipを使う\n",
    "                    chunks.clear()\n",
    "                \n",
    "                else:\n",
    "                    yield []\n",
    "                    \n",
    "            elif i[0] == \"*\":\n",
    "                #*で始まる行は「係受け解析結果」\n",
    "                flag = 0\n",
    "                #chunkのインデックス番号(idx)と係先のインデックス番号(dst)を取得\n",
    "                hs = i.split(\" \")\n",
    "                idx = int(hs[1])\n",
    "                dst = int(hs[2].strip(\"D\"))\n",
    "                \n",
    "                #chunkが無ければ生成し、係先のインデックス番号をセット\n",
    "                if idx not in chunks:\n",
    "                    chunks[idx] = Chunk()\n",
    "                chunks[idx].dst = dst\n",
    "                    \n",
    "                #係先のchunkを生成し、係元のインデックス番号を追加しておく\n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst] = Chunk()\n",
    "                    chunks[dst].srcs.append(idx)\n",
    "                    \n",
    "            else:\n",
    "                #この部分は形態素解析結果\n",
    "                flag = 0\n",
    "                ls = i.split(\"\\t\") \n",
    "                ks = ls[1].split(\",\")\n",
    "                #ls[0]=surface, ks[0]=base, ks[1]=pos, ks[6]=pos1\n",
    "                chunks[idx].morphs.append(Morph(ls[0],ks[6],ks[0],ks[1]))\n",
    "        \n",
    "        #raise StopIteration() #yield文がなくなった時用の例外処理(無くても大丈夫？)\n",
    "    \n",
    "    Analysis_Chunk()\n",
    "    \n",
    "    k = Analysis_Chunk()\n",
    "\n",
    "    l = int(input(\"配列番号で何番目？\"))\n",
    "    \n",
    "    for n,items in enumerate(k):\n",
    "        if n == l:\n",
    "            for n1,item in enumerate(items):\n",
    "                print(\"[{}]{}\".format(n1,item))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第42問"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CaboChaでは、値渡しはやめておく。\n",
    "#fは一応グローバルな変数としておく。\n",
    "#イテレータ・ジェネレータは以下のサイトが詳しい\n",
    "#https://qiita.com/tomotaka_ito/items/35f3eb108f587022fa09\n",
    "\n",
    "import re\n",
    "\n",
    "with open(\"neko.txt.cabocha\",\"r\",encoding=\"utf-8_sig\") as f:\n",
    "    \n",
    "    class Morph:\n",
    "        #表層形,基本形,品詞,品詞細分類1の順\n",
    "        def __init__(self,surface,base,pos,pos1):\n",
    "            self.surface = surface\n",
    "            self.base = base\n",
    "            self.pos = pos\n",
    "            self.pos1 = pos1\n",
    "    \n",
    "        #イテレータ\n",
    "        def __str__(self):\n",
    "            return \"surface:{}\\t base:{}\\t pos:{}\\t pos1:{}\".format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "    class Chunk:\n",
    "        #形態素（Morphオブジェクト）のリスト,係り先文節インデックス番号,係り元文節インデックス番号のリスト（srcs）\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.morphs = []\n",
    "            self.dst = -1 #index番号は-1としておく\n",
    "            self.srcs = []\n",
    "        \n",
    "        def __str__(self):\n",
    "            surface = \"\"\n",
    "            for morph in self.morphs:\n",
    "                surface += morph.surface\n",
    "            return \"words:{}\\t dst:{}\\t srcs:{}\".format(surface,self.dst,self.srcs)\n",
    "        \n",
    "        def clause_text(self):\n",
    "            #句読点を除いた表層形を返す\n",
    "            result = \"\"\n",
    "            for morph in self.morphs:\n",
    "                if morph.pos != \"記号\":\n",
    "                    result = result + morph.surface\n",
    "            return result\n",
    "            \n",
    "    \n",
    "    def Analysis_Chunk():\n",
    "        chunks = dict()\n",
    "        idx = -1 #keyとなるindex number\n",
    "        flag = 0\n",
    "        #flagを導入した理由\n",
    "        #EOSが2回続いた場合、2回目の場所が空リストとなって実行されてしまうため\n",
    "        \n",
    "        for i in f:\n",
    "        \n",
    "            if i == \"EOS\\n\" and flag > 0:\n",
    "                flag = 0\n",
    "                continue\n",
    "            \n",
    "            elif i == \"EOS\\n\":\n",
    "                #一文が終了したことの処理\n",
    "                flag += 1\n",
    "                \n",
    "                if len(chunks) > 0:\n",
    "                    data = sorted(chunks.items(),key=lambda x:x[0]) \n",
    "                    #このように条件指定することで、配列の0番目がKeyとなりSortしてくれる\n",
    "                    yield list(zip(*data))[1] #2データ以上ある時は、zipを使う\n",
    "                    chunks.clear()\n",
    "                \n",
    "                else:\n",
    "                    yield []\n",
    "                    \n",
    "            elif i[0] == \"*\":\n",
    "                #*で始まる行は「係受け解析結果」\n",
    "                flag = 0\n",
    "                #chunkのインデックス番号(idx)と係先のインデックス番号(dst)を取得\n",
    "                hs = i.split(\" \")\n",
    "                idx = int(hs[1])\n",
    "                dst = int(hs[2].strip(\"D\"))\n",
    "                \n",
    "                #chunkが無ければ生成し、係先のインデックス番号をセット\n",
    "                if idx not in chunks:\n",
    "                    chunks[idx] = Chunk()\n",
    "                chunks[idx].dst = dst\n",
    "                    \n",
    "                #係先のchunkを生成し、係元のインデックス番号を追加しておく\n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst] = Chunk()\n",
    "                    chunks[dst].srcs.append(idx)\n",
    "                    \n",
    "            else:\n",
    "                #この部分は形態素解析結果\n",
    "                flag = 0\n",
    "                ls = i.split(\"\\t\") \n",
    "                ks = ls[1].split(\",\")\n",
    "                #ls[0]=surface, ks[0]=base, ks[1]=pos, ks[6]=pos1\n",
    "                chunks[idx].morphs.append(Morph(ls[0],ks[6],ks[0],ks[1]))\n",
    "        \n",
    "        #raise StopIteration() #yield文がなくなった時用の例外処理(無くても大丈夫？)\n",
    "    \n",
    "    Analysis_Chunk()\n",
    "    \n",
    "    k = Analysis_Chunk()\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    #1文ずつリストを作成\n",
    "    for items in k:\n",
    "        for item in items:\n",
    "            #係先があるものだけ取ってくる\n",
    "            if item.dst != -1: #初期化のままではない\n",
    "                src = item.clause_text()\n",
    "                dst = items[item.dst].clause_text() #一文内のその係先のテキストだけ持ってくる\n",
    "                if src != \"\" and dst != \"\":\n",
    "                    #print(\"{}\\t{}\".format(src,dst))\n",
    "                    x.append(src)\n",
    "                    y.append(dst)\n",
    "    \n",
    "    with open(\"neko_result42.txt\",\"w\",encoding = \"utf-8_sig\") as g:\n",
    "        for i in range(len(x)):\n",
    "            g.write(x[i] + \"\\t\" + y[i] + \"\\n\")\n",
    "    \n",
    "    import pickle\n",
    "    with open(\"neko_result42\",\"wb\") as h:\n",
    "        for j in range(len(x)):\n",
    "            pickle.dump(j,h)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第43問"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CaboChaでは、値渡しはやめておく。\n",
    "#fは一応グローバルな変数としておく。\n",
    "#イテレータ・ジェネレータは以下のサイトが詳しい\n",
    "#https://qiita.com/tomotaka_ito/items/35f3eb108f587022fa09\n",
    "\n",
    "import re\n",
    "\n",
    "with open(\"neko.txt.cabocha\",\"r\",encoding=\"utf-8_sig\") as f:\n",
    "    \n",
    "    class Morph:\n",
    "        #表層形,基本形,品詞,品詞細分類1の順\n",
    "        def __init__(self,surface,base,pos,pos1):\n",
    "            self.surface = surface\n",
    "            self.base = base\n",
    "            self.pos = pos\n",
    "            self.pos1 = pos1\n",
    "    \n",
    "        #イテレータ\n",
    "        def __str__(self):\n",
    "            return \"surface:{}\\t base:{}\\t pos:{}\\t pos1:{}\".format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "    class Chunk:\n",
    "        #形態素（Morphオブジェクト）のリスト,係り先文節インデックス番号,係り元文節インデックス番号のリスト（srcs）\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.morphs = []\n",
    "            self.dst = -1 #index番号は-1としておく\n",
    "            self.srcs = []\n",
    "        \n",
    "        def __str__(self):\n",
    "            surface = \"\"\n",
    "            for morph in self.morphs:\n",
    "                surface += morph.surface\n",
    "            return \"words:{}\\t dst:{}\\t srcs:{}\".format(surface,self.dst,self.srcs)\n",
    "        \n",
    "        def clause_text(self):\n",
    "            #句読点を除いた表層形を返す\n",
    "            result = \"\"\n",
    "            for morph in self.morphs:\n",
    "                if morph.pos != \"記号\":\n",
    "                    result = result + morph.surface\n",
    "            return result\n",
    "        \n",
    "        def check_pos(self,pos):\n",
    "            #名詞を含む文節かどうかを判断\n",
    "            for morph in self.morphs:\n",
    "                if morph.pos == pos:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            \n",
    "    \n",
    "    def Analysis_Chunk():\n",
    "        chunks = dict()\n",
    "        idx = -1 #keyとなるindex number\n",
    "        flag = 0\n",
    "        #flagを導入した理由\n",
    "        #EOSが2回続いた場合、2回目の場所が空リストとなって実行されてしまうため\n",
    "        \n",
    "        for i in f:\n",
    "        \n",
    "            if i == \"EOS\\n\" and flag > 0:\n",
    "                flag = 0\n",
    "                continue\n",
    "            \n",
    "            elif i == \"EOS\\n\":\n",
    "                #一文が終了したことの処理\n",
    "                flag += 1\n",
    "                \n",
    "                if len(chunks) > 0:\n",
    "                    data = sorted(chunks.items(),key=lambda x:x[0]) \n",
    "                    #このように条件指定することで、配列の0番目がKeyとなりSortしてくれる\n",
    "                    yield list(zip(*data))[1] #2データ以上ある時は、zipを使う\n",
    "                    chunks.clear()\n",
    "                \n",
    "                else:\n",
    "                    yield []\n",
    "                    \n",
    "            elif i[0] == \"*\":\n",
    "                #*で始まる行は「係受け解析結果」\n",
    "                flag = 0\n",
    "                #chunkのインデックス番号(idx)と係先のインデックス番号(dst)を取得\n",
    "                hs = i.split(\" \")\n",
    "                idx = int(hs[1])\n",
    "                dst = int(hs[2].strip(\"D\"))\n",
    "                \n",
    "                #chunkが無ければ生成し、係先のインデックス番号をセット\n",
    "                if idx not in chunks:\n",
    "                    chunks[idx] = Chunk()\n",
    "                chunks[idx].dst = dst\n",
    "                    \n",
    "                #係先のchunkを生成し、係元のインデックス番号を追加しておく\n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst] = Chunk()\n",
    "                    chunks[dst].srcs.append(idx)\n",
    "                    \n",
    "            else:\n",
    "                #この部分は形態素解析結果\n",
    "                flag = 0\n",
    "                ls = i.split(\"\\t\") \n",
    "                ks = ls[1].split(\",\")\n",
    "                #ls[0]=surface, ks[0]=base, ks[1]=pos, ks[6]=pos1\n",
    "                chunks[idx].morphs.append(Morph(ls[0],ks[6],ks[0],ks[1]))\n",
    "        \n",
    "        #raise StopIteration() #yield文がなくなった時用の例外処理(無くても大丈夫？)\n",
    "    \n",
    "    Analysis_Chunk()\n",
    "    \n",
    "    k = Analysis_Chunk()\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    #1文ずつリストを作成\n",
    "    for items in k:\n",
    "        for item in items:\n",
    "            #係先があるものだけ取ってくる\n",
    "            if item.dst != -1: #初期化のままではない\n",
    "                if (item.check_pos(\"名詞\") == True) and (items[item.dst].check_pos(\"動詞\") == True):\n",
    "                    src = item.clause_text()\n",
    "                    dst = items[item.dst].clause_text() #一文内のその係先のテキストだけ持ってくる\n",
    "                    if src != \"\" and dst != \"\":\n",
    "                        #print(\"{}\\t{}\".format(src,dst))\n",
    "                        x.append(src)\n",
    "                        y.append(dst)\n",
    "    \n",
    "    with open(\"neko_result43.txt\",\"w\",encoding = \"utf-8_sig\") as g:\n",
    "        for i in range(len(x)):\n",
    "            g.write(x[i] + \"\\t\" + y[i] + \"\\n\")\n",
    "    \n",
    "    import pickle\n",
    "    with open(\"neko_result43\",\"wb\") as h:\n",
    "        for j in range(len(x)):\n",
    "            pickle.dump(j,h)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第44問"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "グラフにする文字列を入力----->ああ我こそは偉大なる王様であるぞ。\n"
     ]
    }
   ],
   "source": [
    "with open(\"neko_tmp.txt\",\"w\",encoding = \"utf-8_sig\") as h:\n",
    "    h.write(input(\"グラフにする文字列を入力----->\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"neko_tmp.txt\",\"r\",encoding = \"utf-8_sig\") as f:\n",
    "    with open(\"neko_tmp.txt.cabocha\",\"w\",encoding = \"utf-8_sig\") as g:\n",
    "        c = CaboCha.Parser()\n",
    "        for i in f:\n",
    "            g.write(c.parse(i).toString(CaboCha.FORMAT_LATTICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "(dot.exe:23180): Pango-WARNING **: couldn't load font \"IPAexGothic Not-Rotated 14\", falling back to \"Sans Not-Rotated 14\", expect ugly output.\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CaboChaでは、値渡しはやめておく。\n",
    "#fは一応グローバルな変数としておく。\n",
    "#イテレータ・ジェネレータは以下のサイトが詳しい\n",
    "#https://qiita.com/tomotaka_ito/items/35f3eb108f587022fa09\n",
    "\n",
    "#以下は参照サイト\n",
    "#https://qiita.com/segavvy/items/d1a9a8d87d8dc10a8f15\n",
    "\n",
    "import re\n",
    "import pydot_ng as pydot\n",
    "\n",
    "with open(\"neko_tmp.txt.cabocha\",\"r\",encoding=\"utf-8_sig\") as f:\n",
    "    \n",
    "    class Morph:\n",
    "        #表層形,基本形,品詞,品詞細分類1の順\n",
    "        def __init__(self,surface,base,pos,pos1):\n",
    "            self.surface = surface\n",
    "            self.base = base\n",
    "            self.pos = pos\n",
    "            self.pos1 = pos1\n",
    "    \n",
    "        #イテレータ\n",
    "        def __str__(self):\n",
    "            return \"surface:{}\\t base:{}\\t pos:{}\\t pos1:{}\".format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "    class Chunk:\n",
    "        #形態素（Morphオブジェクト）のリスト,係り先文節インデックス番号,係り元文節インデックス番号のリスト（srcs）\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.morphs = []\n",
    "            self.dst = -1 #index番号は-1としておく\n",
    "            self.srcs = []\n",
    "        \n",
    "        def __str__(self):\n",
    "            surface = \"\"\n",
    "            for morph in self.morphs:\n",
    "                surface += morph.surface\n",
    "            return \"words:{}\\t dst:{}\\t srcs:{}\".format(surface,self.dst,self.srcs)\n",
    "        \n",
    "        def clause_text(self):\n",
    "            #句読点を除いた表層形を返す\n",
    "            result = \"\"\n",
    "            for morph in self.morphs:\n",
    "                if morph.pos != \"記号\":\n",
    "                    result = result + morph.surface\n",
    "            return result\n",
    "        \n",
    "        def check_pos(self,pos):\n",
    "            #名詞を含む文節かどうかを判断\n",
    "            for morph in self.morphs:\n",
    "                if morph.pos == pos:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            \n",
    "    \n",
    "    def Analysis_Chunk():\n",
    "        chunks = dict()\n",
    "        idx = -1 #keyとなるindex number\n",
    "        flag = 0\n",
    "        #flagを導入した理由\n",
    "        #EOSが2回続いた場合、2回目の場所が空リストとなって実行されてしまうため\n",
    "        \n",
    "        for i in f:\n",
    "        \n",
    "            if i == \"EOS\\n\" and flag > 0:\n",
    "                flag = 0\n",
    "                continue\n",
    "            \n",
    "            elif i == \"EOS\\n\":\n",
    "                #一文が終了したことの処理\n",
    "                flag += 1\n",
    "                \n",
    "                if len(chunks) > 0:\n",
    "                    data = sorted(chunks.items(),key=lambda x:x[0]) \n",
    "                    #このように条件指定することで、配列の0番目がKeyとなりSortしてくれる\n",
    "                    yield list(zip(*data))[1] #2データ以上ある時は、zipを使う\n",
    "                    chunks.clear()\n",
    "                \n",
    "                else:\n",
    "                    yield []\n",
    "                    \n",
    "            elif i[0] == \"*\":\n",
    "                #*で始まる行は「係受け解析結果」\n",
    "                flag = 0\n",
    "                #chunkのインデックス番号(idx)と係先のインデックス番号(dst)を取得\n",
    "                hs = i.split(\" \")\n",
    "                idx = int(hs[1])\n",
    "                dst = int(hs[2].strip(\"D\"))\n",
    "                \n",
    "                #chunkが無ければ生成し、係先のインデックス番号をセット\n",
    "                if idx not in chunks:\n",
    "                    chunks[idx] = Chunk()\n",
    "                chunks[idx].dst = dst\n",
    "                    \n",
    "                #係先のchunkを生成し、係元のインデックス番号を追加しておく\n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst] = Chunk()\n",
    "                    chunks[dst].srcs.append(idx)\n",
    "                    \n",
    "            else:\n",
    "                #この部分は形態素解析結果\n",
    "                flag = 0\n",
    "                ls = i.split(\"\\t\") \n",
    "                ks = ls[1].split(\",\")\n",
    "                #ls[0]=surface, ks[0]=base, ks[1]=pos, ks[6]=pos1\n",
    "                chunks[idx].morphs.append(Morph(ls[0],ks[6],ks[0],ks[1]))\n",
    "        \n",
    "        #raise StopIteration() #yield文がなくなった時用の例外処理(無くても大丈夫？)\n",
    "    \n",
    "    def Making_Tree(tree_list,direct=True):\n",
    "        \n",
    "        if direct == True:\n",
    "            graph = pydot.Dot(graph_type=\"graph\")\n",
    "        else:\n",
    "            graph = pydot.Dot(graph_type=\"digraph\")\n",
    "            \n",
    "        for tree in tree_list:\n",
    "            \n",
    "            #node用のidとlabelの用意\n",
    "            id1 = str(tree[0][0])\n",
    "            text1 = str(tree[0][1])\n",
    "            id2 = str(tree[1][0])\n",
    "            text2 = str(tree[1][1])\n",
    "            \n",
    "            #ノードの追加\n",
    "            graph.add_node(pydot.Node(id1,label=text1,fontname=\"IPAexGothic\"))\n",
    "            graph.add_node(pydot.Node(id2,label=text2,fontname=\"IPAexGothic\"))\n",
    "            \n",
    "            #エッジの追加\n",
    "            graph.add_edge(pydot.Edge(id1,id2))\n",
    "            \n",
    "        return graph\n",
    "    \n",
    "    \n",
    "    k = Analysis_Chunk()\n",
    "    \n",
    "    #1文ずつリストを作成\n",
    "    for items in k:\n",
    "        \n",
    "        trees = []\n",
    "        \n",
    "        for i,item in enumerate(items): #2つの値を取ってきたい⇒リスト化\n",
    "            #係先があるものだけ取ってくる\n",
    "            if item.dst != -1: #初期化のままではない\n",
    "                src = item.clause_text()\n",
    "                dst = items[item.dst].clause_text() #一文内のその係先のテキストだけ持ってくる\n",
    "                if src != \"\" and dst != \"\":\n",
    "                    #print(\"{}\\t{}\".format(src,dst))\n",
    "                    trees.append(((i,src),(item.dst,dst))) #idはi,item.dst , #label（モノ）はsrc,dst\n",
    "        \n",
    "        if len(trees) > 0:\n",
    "            graph = Making_Tree(trees,direct=True)\n",
    "            graph.write_png(\"result44.png\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第45問"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CaboChaでは、値渡しはやめておく。\n",
    "#fは一応グローバルな変数としておく。\n",
    "#イテレータ・ジェネレータは以下のサイトが詳しい\n",
    "#https://qiita.com/tomotaka_ito/items/35f3eb108f587022fa09\n",
    "\n",
    "import re\n",
    "\n",
    "with open(\"neko.txt.cabocha\",\"r\",encoding=\"utf-8_sig\") as f:\n",
    "    \n",
    "    class Morph:\n",
    "        #表層形,基本形,品詞,品詞細分類1の順\n",
    "        def __init__(self,surface,base,pos,pos1):\n",
    "            self.surface = surface\n",
    "            self.base = base\n",
    "            self.pos = pos\n",
    "            self.pos1 = pos1\n",
    "    \n",
    "        #イテレータ\n",
    "        def __str__(self):\n",
    "            return \"surface:{}\\t base:{}\\t pos:{}\\t pos1:{}\".format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "    class Chunk:\n",
    "        #形態素（Morphオブジェクト）のリスト,係り先文節インデックス番号,係り元文節インデックス番号のリスト（srcs）\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.morphs = []\n",
    "            self.dst = -1 #index番号は-1としておく\n",
    "            self.srcs = []\n",
    "        \n",
    "        def __str__(self):\n",
    "            surface = \"\"\n",
    "            for morph in self.morphs:\n",
    "                surface += morph.surface\n",
    "            return \"words:{}\\t dst:{}\\t srcs:{}\".format(surface,self.dst,self.srcs)\n",
    "        \n",
    "        def clause_text(self):\n",
    "            #句読点を除いた表層形を返す\n",
    "            result = \"\"\n",
    "            for morph in self.morphs:\n",
    "                if morph.pos != \"記号\":\n",
    "                    result = result + morph.surface\n",
    "            return result\n",
    "        \n",
    "        def check_pos(self,pos):\n",
    "            #名詞を含む文節かどうかを判断\n",
    "            for morph in self.morphs:\n",
    "                if morph.pos == pos:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "        \n",
    "        #指定した品詞と品詞細分類のリストを返す。\n",
    "        #品詞細分類の指定が無ければ、品詞のみでも判定\n",
    "        def get_morphs_by_pos(self,pos,pos1=\"\"):\n",
    "            \n",
    "            result = []\n",
    "            \n",
    "            if len(pos1) > 0:\n",
    "                for i in self.morphs:\n",
    "                    if (i.pos == pos) and (i.pos1 == pos1):\n",
    "                        result.append(i)\n",
    "            \n",
    "            else:\n",
    "                for i in self.morphs:\n",
    "                    if i.pos == pos:\n",
    "                        result.append(i)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "            \n",
    "    def Analysis_Chunk():\n",
    "        chunks = dict()\n",
    "        idx = -1 #keyとなるindex number\n",
    "        flag = 0\n",
    "        #flagを導入した理由\n",
    "        #EOSが2回続いた場合、2回目の場所が空リストとなって実行されてしまうため\n",
    "        \n",
    "        for i in f:\n",
    "        \n",
    "            if i == \"EOS\\n\" and flag > 0:\n",
    "                flag = 0\n",
    "                continue\n",
    "            \n",
    "            elif i == \"EOS\\n\":\n",
    "                #一文が終了したことの処理\n",
    "                flag += 1\n",
    "                \n",
    "                if len(chunks) > 0:\n",
    "                    data = sorted(chunks.items(),key=lambda x:x[0]) \n",
    "                    #このように条件指定することで、配列の0番目がKeyとなりSortしてくれる\n",
    "                    yield list(zip(*data))[1] #2データ以上ある時は、zipを使う\n",
    "                    chunks.clear()\n",
    "                \n",
    "                else:\n",
    "                    yield []\n",
    "                    \n",
    "            elif i[0] == \"*\":\n",
    "                #*で始まる行は「係受け解析結果」\n",
    "                flag = 0\n",
    "                #chunkのインデックス番号(idx)と係先のインデックス番号(dst)を取得\n",
    "                hs = i.split(\" \")\n",
    "                idx = int(hs[1])\n",
    "                dst = int(hs[2].strip(\"D\"))\n",
    "                \n",
    "                #chunkが無ければ生成し、係先のインデックス番号をセット\n",
    "                if idx not in chunks:\n",
    "                    chunks[idx] = Chunk()\n",
    "                chunks[idx].dst = dst\n",
    "                    \n",
    "                #係先のchunkを生成し、係元のインデックス番号を追加しておく\n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst] = Chunk()\n",
    "                    chunks[dst].srcs.append(idx)\n",
    "                    \n",
    "            else:\n",
    "                #この部分は形態素解析結果\n",
    "                flag = 0\n",
    "                ls = i.split(\"\\t\") \n",
    "                ks = ls[1].split(\",\")\n",
    "                #ls[0]=surface, ks[0]=base, ks[1]=pos, ks[6]=pos1\n",
    "                chunks[idx].morphs.append(Morph(ls[0],ks[6],ks[0],ks[1]))\n",
    "        \n",
    "        #raise StopIteration() #yield文がなくなった時用の例外処理(無くても大丈夫？)\n",
    "    \n",
    "    Analysis_Chunk()\n",
    "    \n",
    "    k = Analysis_Chunk()\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    #1文ずつリストを作成\n",
    "    for items in k:\n",
    "        for item in items:\n",
    "            #動詞があるかどうかのチェック\n",
    "            verbs = item.get_morphs_by_pos(\"動詞\")\n",
    "            \n",
    "            if len(verbs) < 1:\n",
    "                continue\n",
    "            \n",
    "            #係元を列挙\n",
    "            prts = []\n",
    "            for src in item.srcs:\n",
    "                #助詞の検索\n",
    "                prts_in_item = items[src].get_morphs_by_pos(\"助詞\")\n",
    "                if len(prts_in_item) > 1:\n",
    "                    #助詞が2つ以上ある場合\n",
    "                    #助詞よりも格助詞を優先\n",
    "                    \n",
    "                    kaku_prts = items[src].get_morphs_by_pos(\"助詞\",\"格助詞\")\n",
    "            \n",
    "                    if len(kaku_prts) > 0:\n",
    "                        prts_in_item = kaku_prts\n",
    "                \n",
    "                if len(prts_in_item) > 0:\n",
    "                    prts.append(prts_in_item[-1])\n",
    "            \n",
    "            if len(prts) < 1:\n",
    "                continue\n",
    "                \n",
    "            x.append(\"{}\\t{}\\n\".format(verbs[0].base,\" \".join(sorted(prt.surface for prt in prts))))\n",
    "    \n",
    "    with open(\"neko_result45.txt\",\"w\",encoding = \"utf-8_sig\") as g:\n",
    "        for i in range(len(x)):\n",
    "            g.write(x[i])\n",
    "            \n",
    "    import pickle\n",
    "    with open(\"neko_result45\",\"wb\") as h:\n",
    "        for i in range(len(x)):\n",
    "            pickle.dump(i,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第46問"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CaboChaでは、値渡しはやめておく。\n",
    "#fは一応グローバルな変数としておく。\n",
    "#イテレータ・ジェネレータは以下のサイトが詳しい\n",
    "#https://qiita.com/tomotaka_ito/items/35f3eb108f587022fa09\n",
    "\n",
    "import re\n",
    "\n",
    "with open(\"neko.txt.cabocha\",\"r\",encoding=\"utf-8_sig\") as f:\n",
    "    \n",
    "    class Morph:\n",
    "        #表層形,基本形,品詞,品詞細分類1の順\n",
    "        def __init__(self,surface,base,pos,pos1):\n",
    "            self.surface = surface\n",
    "            self.base = base\n",
    "            self.pos = pos\n",
    "            self.pos1 = pos1\n",
    "    \n",
    "        #イテレータ\n",
    "        def __str__(self):\n",
    "            return \"surface:{}\\t base:{}\\t pos:{}\\t pos1:{}\".format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "    class Chunk:\n",
    "        #形態素（Morphオブジェクト）のリスト,係り先文節インデックス番号,係り元文節インデックス番号のリスト（srcs）\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.morphs = []\n",
    "            self.dst = -1 #index番号は-1としておく\n",
    "            self.srcs = []\n",
    "        \n",
    "        def __str__(self):\n",
    "            surface = \"\"\n",
    "            for morph in self.morphs:\n",
    "                surface += morph.surface\n",
    "            return \"words:{}\\t dst:{}\\t srcs:{}\".format(surface,self.dst,self.srcs)\n",
    "        \n",
    "        def clause_text(self):\n",
    "            #句読点を除いた表層形を返す\n",
    "            result = \"\"\n",
    "            for morph in self.morphs:\n",
    "                if morph.pos != \"記号\":\n",
    "                    result = result + morph.surface\n",
    "            return result\n",
    "        \n",
    "        def check_pos(self,pos):\n",
    "            #名詞を含む文節かどうかを判断\n",
    "            for morph in self.morphs:\n",
    "                if morph.pos == pos:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "        \n",
    "        #指定した品詞と品詞細分類のリストを返す。\n",
    "        #品詞細分類の指定が無ければ、品詞のみでも判定\n",
    "        def get_morphs_by_pos(self,pos,pos1=\"\"):\n",
    "            \n",
    "            result = []\n",
    "            \n",
    "            if len(pos1) > 0:\n",
    "                for i in self.morphs:\n",
    "                    if (i.pos == pos) and (i.pos1 == pos1):\n",
    "                        result.append(i)\n",
    "            \n",
    "            else:\n",
    "                for i in self.morphs:\n",
    "                    if i.pos == pos:\n",
    "                        result.append(i)\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        def get_jyosi(self):\n",
    "            \n",
    "            #助詞を1つ返す。\n",
    "            #前問の一部を関数化\n",
    "            \n",
    "            prts = self.get_morphs_by_pos(\"助詞\")\n",
    "            if len(prts) > 1:\n",
    "                kaku_prts = self.get_morphs_by_pos(\"助詞\",\"格助詞\")\n",
    "                if len(kaku_prts) > 0:\n",
    "                    prts = kaku_prts\n",
    "                    \n",
    "            if len(prts) > 0:\n",
    "                return prts[-1].surface\n",
    "            else:\n",
    "                return \"\"\n",
    "\n",
    "            \n",
    "    def Analysis_Chunk():\n",
    "        chunks = dict()\n",
    "        idx = -1 #keyとなるindex number\n",
    "        flag = 0\n",
    "        #flagを導入した理由\n",
    "        #EOSが2回続いた場合、2回目の場所が空リストとなって実行されてしまうため\n",
    "        \n",
    "        for i in f:\n",
    "        \n",
    "            if i == \"EOS\\n\" and flag > 0:\n",
    "                flag = 0\n",
    "                continue\n",
    "            \n",
    "            elif i == \"EOS\\n\":\n",
    "                #一文が終了したことの処理\n",
    "                flag += 1\n",
    "                \n",
    "                if len(chunks) > 0:\n",
    "                    data = sorted(chunks.items(),key=lambda x:x[0]) \n",
    "                    #このように条件指定することで、配列の0番目がKeyとなりSortしてくれる\n",
    "                    yield list(zip(*data))[1] #2データ以上ある時は、zipを使う\n",
    "                    chunks.clear()\n",
    "                \n",
    "                else:\n",
    "                    yield []\n",
    "                    \n",
    "            elif i[0] == \"*\":\n",
    "                #*で始まる行は「係受け解析結果」\n",
    "                flag = 0\n",
    "                #chunkのインデックス番号(idx)と係先のインデックス番号(dst)を取得\n",
    "                hs = i.split(\" \")\n",
    "                idx = int(hs[1])\n",
    "                dst = int(hs[2].strip(\"D\"))\n",
    "                \n",
    "                #chunkが無ければ生成し、係先のインデックス番号をセット\n",
    "                if idx not in chunks:\n",
    "                    chunks[idx] = Chunk()\n",
    "                chunks[idx].dst = dst\n",
    "                    \n",
    "                #係先のchunkを生成し、係元のインデックス番号を追加しておく\n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst] = Chunk()\n",
    "                    chunks[dst].srcs.append(idx)\n",
    "                    \n",
    "            else:\n",
    "                #この部分は形態素解析結果\n",
    "                flag = 0\n",
    "                ls = i.split(\"\\t\") \n",
    "                ks = ls[1].split(\",\")\n",
    "                #ls[0]=surface, ks[0]=base, ks[1]=pos, ks[6]=pos1\n",
    "                chunks[idx].morphs.append(Morph(ls[0],ks[6],ks[0],ks[1]))\n",
    "        \n",
    "        #raise StopIteration() #yield文がなくなった時用の例外処理(無くても大丈夫？)\n",
    "    \n",
    "    Analysis_Chunk()\n",
    "    \n",
    "    k = Analysis_Chunk()\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    #1文ずつリストを作成\n",
    "    for items in k:\n",
    "        for item in items:\n",
    "            #動詞があるかどうかのチェック\n",
    "            verbs = item.get_morphs_by_pos(\"動詞\")\n",
    "            \n",
    "            if len(verbs) < 1:\n",
    "                continue\n",
    "            \n",
    "            #係元に助詞を含む文節を列挙\n",
    "            items_include_prt = []\n",
    "            for src in item.srcs:\n",
    "                if len(items[src].get_jyosi()) > 0:\n",
    "                    items_include_prt.append(items[src])\n",
    "                if len(items_include_prt) < 1:\n",
    "                    continue\n",
    "                \n",
    "                #辞書順でソート\n",
    "                items_include_prt.sort(key=lambda x:x.get_jyosi())\n",
    "                \n",
    "            x.append(\"{}\\t{}\\t{}\\n\".format(verbs[0].base, \" \".join(sorted(item.get_jyosi() for item in items_include_prt)), \" \".join(sorted(item.clause_text() for item in items_include_prt))))\n",
    "    \n",
    "    with open(\"neko_result46.txt\",\"w\",encoding = \"utf-8_sig\") as g:\n",
    "        for i in range(len(x)):\n",
    "            g.write(x[i])\n",
    "            \n",
    "    import pickle\n",
    "    with open(\"neko_result46\",\"wb\") as h:\n",
    "        for i in range(len(x)):\n",
    "            pickle.dump(i,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第47問"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第48問"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第49問"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
